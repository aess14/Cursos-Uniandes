
# Corte 1

### 1

### 2

### 3

### 4

### 5

* Escriba un código para muestrear la distribución exponencial inversa ((1/lambda)(1/x**2)exp(-lambda/x)) con el algoritmo de Metropolis-Hastings.
Prepare una gráfica con el histograma de la cadena de Markov para verificar que tiene la respuesta correcta.

### 6 (Para Calificar)

* Escriba un codigo para ajustar datos bidimensionales a una parabola usando 
estadistica Bayesiana y el algoritmo de Metropolis Hastings.

# Corte 2

### 7 

* Escriba u codigo para ajustar datos bidimensionales a una parabola usando 
estadistica Bayesiana y el algoritmo de Monte Carlo Hamiltoniano.

### 8 (Para Calificar)

* A través de fotos aéreas se tienen medidas del alcance de un
  lanzador de proyectiles. Se tienen cinco valores (en metros): 880,
  795, 782, 976, 178. Todas las mediciones tienen una incertidumbre de
  5 metros. Los valores diferentes del alcance se deben a diferentes
  ángulos de lanzamiento, la velocidad inicial es aproximadamente la
  misma. Utilice el teorema de Bayes y el método de
  Metropolis-Hastings para encontrar la distribución de probabilidad
  de la velocidad inicial dados los datos observacionales. La
  respuesta debe estar implementada dentro de un archivo .py. El
  código debe producir una única gráfica de la distribución de
  probabilidad pedidad donde en el título se anotan la velocidad que
  maximiza la probabilidad, el valor medio y la desviación estándar.  
  
  
 ### 9
 
 ### 10
 
 ### 11
 
 Utilizando los datos en `data/fitting.txt` calcule la evidencia bayesiana para M modelos diferentes, donde cada modelo es un polinomio de orden m. Prepare una grafica de la evidencia como funcion del orden del polinomio. El prior para los coeficientes del polinomio debe ser uniforme entre -1 y 1, y  el orden de los polinomios debe variar entre 0<=m<=20.
 
 ### 12 
 
 Reproduzca el panel derecho la Figura 2.9 de ISLR (solamente las curvas de test y training) utilizando los datos de `fitting.txt` y métodos MCMC para encontrar los mejor parámetros de los modelos polinomiales de grado cero hasta diez.
 
 # Corte 3
 
 ### 13

Ejercicios del notebook `06.Regresion_lineal/regresion_lineal.ipynb`
 
 ### 14
 
Ejercicios del notebook `07.CrossValidation/bootstrap.ipynb`

### 15

Implemente (i.e. no reutilice funciones de scikit-learn):

1. Leave-One-Out Cross-Validation

2. k-Fold Cross-Validation

para calcular el error cuadrático medio como función del orden polinomial de un ajuste de `Price` como funcion de `Horsepower`. Intente con polinomios de orden 1 hasta 10. Compare esas dos graficas con el Criterio de informacion Bayesiana como funcion del orden del polinomio.

La solución debe estar en un notebook que asuma que el archivo de datos se encuentra en el mismo directorio que el notebook.

### 16 (Para Calificar)

Usando como target a 'Price' y como variables ['MPG.city', 'MPG.highway', 'EngineSize',    'Horsepower', 'RPM', 'Rev.per.mile',   'Fuel.tank.capacity', 'Passengers', 'Length',   'Wheelbase', 'Width', 'Turn.circle', 'Weight'], el objetivo de este ejercicio es elegir el conjunto de variables que mayor influencia tiene sobre el target (asumiendo un modelo lineal). Para esto van a utilizar regularizacion con LASSO y Leave-One-Out cross-validation. El objetivo es reproducir el equivalente de la Figura 6.12 y reportar la combinacion de variables que más influencia tiene. Pueden utilizar sklearn.linear_model.Lasso() y sklearn.model_selection.LeaveOneOut().

# Cuarto corte

### 17 

(primera parte)

Usando los siguientes datos: 

https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Default.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Default.csv

Reproduzca las siguientes Figuras del texto guia:

  - Figura 4.1.
  - Panel derecho Figura 4.2.
  - Figura 4.3.

Haga los ajustes usando scikit-learn http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

(segunda parte)

Divida los datos anteriores en training y test. Calcule la matriz de confusión para esos dos conjuntos de datos. Diría que estos resultados son buenos para predecir Default?

Usando los siguientes datos: 
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Smarket.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Smarket.csv

Dividida también los datos en training y test, en training van todos los datos hasta el año 2004, en test van los demás.
Use la regresión logística para predecir `Direction` en función de las demás variables.
Calcule la matriz de confusión para esos dos conjuntos de datos. Diría que estos resultados son buenos para predecir `Direction`?

## 18

(primera parte)

a. Utilice Linear Discriminant Analysis (LDA)para reproducir las Figuras 4.7 y 4.8 del texto guia.

b. Utilizando los datos de `Default` construya una curva de precision-recall con los resultados del modelo regresión logística (http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html). *Escriba su propia función. No utilice la de sklearn*

c. Utilizando el siguiente dataset 

https://vincentarelbundock.github.io/Rdatasets/doc/MASS/Boston.html
https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv

encuentre los mejores predictores para saber si un suburbio tiene una rata de crimen mayor o menor a la media.
Utilice LDA y regresion logistica junto a curvas ROC y Precision-Recall para justificar su respuesta.

(segunda parte)

d. Utilizando el siguiente dataset

https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Auto.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Auto.csv

repita los mismos pasos del punto c. para predecir si un carro tiene `mpg` mayor o menor a la media.

## 19

(Primera parte)

a. Partiendo del siguiente dataset
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Hitters.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Hitters.csv

utilice funciones de sklearn para reproducir la Figura 8.5 del texto guia.

b. Partiendo del siguiente dataset
https://archive.ics.uci.edu/ml/datasets/Heart+Disease

utilice funciones de sklearn para reproducir el panel izquierdo de la Figura 8.6.

(Segunda Parte)

c. Partiendo del siguiente dataset
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/OJ.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/OJ.csv

separe los datos en training+validation y test para encontrar las características
del árbol que mejor predice `Purchase`. Tome en cuenta todos los predictores salvo `Store7`.
Utilice las curvas ROC y Precision-Recall como el criterio para encontrar el mejor árbol.

# 20 (Para Calificar - bono de 10%)

(Primera parte)

a) Reproduzca la Figura 8.8 del texto guia.

b) Reproduzca la Figura 8.10 del texto guia. Utilice el dataset `OJ`. 

(Segunda parte)

c) Utilice el data set `XX` para encontrar cual de los siguientes modelos logra predecir mejor el target `YY`:
  - Logistic regresion.
  - Linear discriminant analysis.
  - Classification Tree.
  - Random Forest.
  Haga explicito el criterio que utiliza para comparar los modelos.

# Quinto corte

### 21

(Primera parte)


a)  Sin utilizar funciones de sklearn, utilice el data set `OJ` para encontrar la recta en el espacio de parametros `LoyalCH`-`PriceDiff` que mejor logra dividir `Purchase` en dos clases. 

b) Resuelva el mismo problema anterior pero utilizando Linear Support Vector Classifier de sklearn http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC para encontrar la mejor rectar de separación.

c) Para cada uno de los dos casos anteriores prepare una gráfica como la Figura 9.8 para mostrar los resultados de los dos clasificadores.

(Segunda parte)

d) Utilice el dataset `Cars93.csv` para crear una variable binaria para los
carros que tienen `MPG.city` mayor/menor que la media.
Utilizando `Fuel.tank.capacity`, `Horsepower`, `Length`, `Rev.per.mile`, `Turn.circle`, `Weight` como predictores encuentre (con y sin sklearn) el mejor 
hiperplano que separa los carros en `MPG.city` mayor/menor que la media. 


### 22

(primera parte)

a) utilice el dataset OJ para predecir purchase. para esto utilice SVM con:
 - kernel lineal, encontrando el mejor parametro C usando cross validation.
 - kernel radial  encontrando el mejor parametro gamma usando cross validation.
 
 (segunda parte)
 
 b) Utilice el dataset `spam7` para predecir `yesno`. Para esto utilice SVM con:
 
 - kernel lineal (encontrando el mejor parametro C usando cross validation.)
 - kernel radial (encontrando el mejor parametro gamma usando cross validation.)
 
 con Precision, Recall, TF1 como los criterios de seleccion.

https://vincentarelbundock.github.io/Rdatasets/datasets.html
 
### 23

(primera parte)

a. Reproducir la Figura 10.1 del texto guia.

b. Reproducir la Figura 10.4 del texto guia.

El dataset se puede bajar de esta pagina https://vincentarelbundock.github.io/Rdatasets/datasets.html

(segunda parte)
 
 c. Utilizando el dataset `Cars93` y las variables `'MPG.highway',
       'Cylinders', 'EngineSize', 'Horsepower', 'RPM', 'Rev.per.mile', 'Fuel.tank.capacity', 'Length',
       'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room', 'Luggage.room',
       'Weight'` encuentre las componentes principales que expresan el 80% de la varianza.
 
### 24 (Para Calificar)

(primera parte)

a. Encuentre el número óptimo del clusters en el dataset `dengue` usando como features `'humid', 'temp' , 'h10pix', 'trees' , 'Xmin', 'Ymin'`
    El dataset set puede bajar de esta pagina https://vincentarelbundock.github.io/Rdatasets/datasets.html
    Utilice http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html para encontrar los clusters.
(segunda parte)

(segunda parte)

b. Utilizando el siguiente dataset https://www.kaggle.com/xvivancos/transactions-from-a-bakery encuentre patrones de compras a partir de métodos PCA y k-mean clustering.

(nuevo lugar del dataset https://github.com/hugotrigueiro/Dataset-Transactions-from-a-Bakery-Exploratory-Data-Analysis-/tree/master/Transactions-from-a-Bakery)

# Sexto corte

### 25 

(primera parte)

La respuesta a este ejercicio debe estar en un repositorio en github llamado `NombreApellido_Ejercicio25`

 a) Escriba un programa en C o C++ (`sample.c`) que al ejecutarse como `./sample N mu sigma`, donde `N` es un entero, genere `N` puntos de 
una distribución gaussiana centrada en `mu` y desviación estándar `sigma`. El código debe escribir los resultados en un archivo de nombre
`sample.dat`. Internamente los `N` puntos de deben almacenar primero en un array (puntero) de tamaño `N` antes de ser escritos al archivo.
**No utilice funciones de libreria que ya devuelvan los números con distribución gaussiana.**

b) Escriba un programa en python que lea los resultados de `sample.dat`, haga un histograma normalizado de los datos y lo compare con
la distribución gaussiana analítica con parámetros `mu` y `sigma` estimados a partir de los datos mismos. El programa debe producir
la gráfica `sample.pdf`

c) Escriba un makefile que automatice de manera adecuada las tareas de los puntos a) y b).

(segunda parte)

d) Usando el documento que describe el uso del cluster, escriba un script para que el makefile se pueda ejecutar en la cola de trabajos.

### 26 

(primera parte)

La respuesta a este ejercicio debe estar en un repositorio en github llamado `NombreApellido_Ejercicio26`

 a) Escriba un programa en C o C++ (`cuenta.c`) que al ejecutarse como `./cuenta.x archivo.txt cuenta.txt tiempo.txt` cuente el número de veces que se encuentran las cadenas de caracteres `1`, `12`, `123`, `1234`, `12345` en el archivo `archivo.txt`. El código debe escribir el resultado en el archivo de texto `cuenta.txt` (una sola columna con los 5 números buscados) y en  el archivo `tiempo.txt` el número de segundos que le tomó hacer esa tarea. 
 
b) Pruebe su código con el siguiente archivo: https://github.com/ComputoCienciasUniandes/MetodosComputacionalesDatos/blob/master/homework/2014-20/hw_1/Pi_2500000.txt
 
c) Reparta el contenido del archivo anterior en 10 partes iguales guárdelas en archivos diferentes. Luego corra `cuenta.x` en cada una de las partes. Cuánto se demora en correr en las 10 partes por separado? Haga el mismo ejercicio diviendo el archivo en 20, 50 y 100 partes. Resuma sus resultados en una gráfica del tiempo que toma procesar cada parte como función del número de partes en el que se dividió el archivo original.

(segunda parte)

d) Repita la misma grafica del caso c) para las cuatro situaciones en el cluster:  1nodo:1ppn, 1 nodo:2ppn, 1nodo:4ppn, 1nodo:8ppn.
La gráfica debe incluir entonces cuatro curvas diferentes correspondientes a cada situación.

### 27

(primera parte)

La respuesta a este ejercicio debe estar en un repositorio en github llamado `NombreApellido_Ejercicio27`
El repositorio debe contener el código fuente (`.c`), el makefile para compilar y el script de torque para ejecutar por separado
los siguientes códigos del texto guía

a) Listing 12.2

b) Listing 12.3

(segunda parte)


 c) Escriba un programa en C o C++ (`sample.c`) que al ejecutarse como
 `mpiexec -np n ./sample N mu sigma`, donde `N` es un entero, genere
 `N` puntos de  una distribución gaussiana centrada en `mu` y
 desviación estándar `sigma`. 
 El código debe escribir los resultados en `n` archivos diferentes
 `sample_1.dat`... `sample_n.dat`, donde cada archivo es escrito por
un procesador diferente. 
Cada procesador debe generar y escribir `N/n` puntos. 
Por ejemplo, si `N=2000` y `n=4` cada procesador debe generar y
 escribir `500` puntos. 

**No utilice funciones de libreria que ya devuelvan los números con
 distribución gaussiana.** 

d) Escriba un programa en python que lea todos los archivos
`sample_*.dat` y haga un histograma normalizado de los datos y lo compare
con  la distribución gaussiana analítica con parámetros `mu` y `sigma` estimados a partir de los datos mismos. El programa debe producir
la gráfica `sample.pdf`. Este programa debe funcionar para cualquier `n` que se haya utilizado en el programa de `C`.

e) Incluya en el repositorio un script de torque para:
   - compilar el codigo.
   - ejecutar el codigo.
   - generar la grafica final.


### 28

(primera parte)

La respuesta a este ejercicio debe estar en un repositorio en github llamado `NombreApellido_Ejercicio28`
El repositorio debe contener el código fuente (`.c`), el makefile para compilar y el script de torque para ejecutar por separado
el siguientes códigos del texto guía

a) Listing 12.7

(segunda parte)

b) Escriba un programa en C o C++ (`integra.c`) que al ejecutarse como
 `mpiexec -np n ./integra.x N `, donde `N` es un entero, genere
 `N` puntos para calcular la integral descrita en SICUA por métodos Monte Carlo (ver https://github.com/ComputoCienciasUniandes/MetodosComputacionalesAvanzados/blob/master/secciones/05.MCMC/01_Motivacion_Integracion.ipynb).
 Cada procesador debe generar y utilizar `N/n` puntos y calcular el promedio correspondiente.
 Al final el procesador de rango `0` debe promediar los promedios de cada procesador
 y escribir el resultado final en la salida estándar.
 
 c) El script de torque correspondiente debe:
  - compilar el codigo.
  - ejecutar el codigo para `N=1E3, 1E4, 1E5, 1E6, 1E7, 1E8` y guardar los resultados en diferentes archivos de texto.
  - ejecutar un codigo de python que lea los resultados de esas integrales para diferentes valores de `N` y grafique
    en escala logaritmica el error porcentual entre la solución numérica y analítica como función de `N`.

### 29


La respuesta a este ejercicio debe estar en un repositorio en github llamado `NombreApellido_Ejercicio29`
El repositorio debe contener el código fuente (`.c`), y un script de torque para compilar y ejecutar los siguientes
códigos.

(primera parte)

a) Un código que implemente `HelloWorld.C` como muestra el texto guía en la sección 13.3.

b) Un código (serial) en C que resuelva la ecuación de advección no lineal tal como se muestra aquí: https://github.com/ComputoCienciasUniandes/MetodosComputacionalesAvanzados/blob/master/weeks/05/code/non_linear_advection.py y prepare
una gráfica en python con la condición inicial y el resultado final en la misma figura.
(breve introducción a advección no lineal https://en.wikipedia.org/wiki/Burgers%27_equation)


### 30

### Final

El final tiene 120 puntos en total. 100 puntos corresponden a una nota de 5.0.
Las respuestas al final deben estar en un repositorio llamado `NombreApellido_Final`

1) (30 puntos) Parcicipar con un notebook en este challenge de Kaggle: https://www.kaggle.com/kaggle/kaggle-survey-2018.
   Guardar el notebook en el repositorio. **Deadline: Lunes 3 de Diciembre**.

